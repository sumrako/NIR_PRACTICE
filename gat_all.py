# -*- coding: utf-8 -*-
"""GAT_all.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_HBcfq5iV8Q30QCHZaUdsSrVo8ee0nKW
"""

!pip install finnhub-python

"""## Создание матрицы весов на основе корреляции"""

import yfinance as yf
import pandas as pd
import finnhub

ticker = 'TSLA'
train_start_date = "2023-06-01"
train_end_date = "2023-07-01"

finnhub_client = finnhub.Client(api_key="ch7r1q9r01qhapm5elf0ch7r1q9r01qhapm5elfg")

# список тикеров peers-компаний
concurrency_tickers = finnhub_client.company_peers(ticker)

# получаем данные по ценам акций компаний
prices = yf.download(concurrency_tickers, period="1y")['Close']

# находим корреляцию между ценами акций компаний
correlation_matrix = prices.corr()

# формируем матрицу весов на основе корреляций
weights_matrix = pd.DataFrame(columns=concurrency_tickers, index=concurrency_tickers)
for i in range(len(concurrency_tickers)):
    for j in range(len(concurrency_tickers)):
        if i == j:
            weights_matrix.iloc[i,j] = 1
        else:
            weights_matrix.iloc[i,j] = correlation_matrix.iloc[i,j]

print(weights_matrix)

from sec_api import QueryApi

api = QueryApi(api_key='')
params = {
    'tickers': concurrency_tickers,
    'date_from': train_start_date,
    'date_to': train_end_date,
    'limit': 200
}

transactions_matrix = []

response = api.transactions(params=params)
transactions = response['data']
for transaction in transactions:
    if transaction['partner'] in concurrency_tickers and transaction['source'] in concurrency_tickers:
        transactions_matrix.append([transaction['source'], transaction['partner']], transaction['timestamp'])

import numpy as np
from tick.hawkes import SimuHawkesExpKernels, HawkesKernelExp
from tick.base import TimeFunction

# Define the companies and their corresponding IDs
num_companies = len(concurrency_tickers)

# Define the adjacency matrix to store the graph weights
adjacency_matrix = np.zeros((num_companies, num_companies))

# Define the Hawkes process parameters
decay = 0.1  # Decay parameter

# Create the Hawkes process object
hawkes = SimuHawkesExpKernels(adjacency=adjacency_matrix, decays=decay, verbose=False)

# Generate the transaction data for the companies
# You will need to replace this with your own data or logic to generate transactions
# Each row represents a transaction: [company_id1, company_id2, timestamp]
transactions = transaction_matrix

# Add the transactions to the Hawkes process
for transaction in transactions:
    company_id1, company_id2, timestamp = transaction
    hawkes.track_timestamp(company_id1, company_id2, timestamp)

# Estimate the parameters of the Hawkes process
hawkes.fit()

# Get the estimated adjacency matrix
adjacency_matrix = hawkes.adjacency

print(adjacency_matrix)

"""## Получение новостных данных и их обработка"""

!pip install transformers

import torch
from transformers import RobertaTokenizer, RobertaModel

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
roberta = RobertaModel.from_pretrained('roberta-base')

import datetime

date_index = pd.date_range(start=train_start_date, end=train_end_date, freq='D')
encode_news_df = pd.DataFrame('', index=date_index, columns=concurrency_tickers)

for сoncurrency_ticker in concurrency_tickers:
      news = finnhub_client.company_news(сoncurrency_ticker, _from=train_start_date, to=train_end_date)
      for new in news:
          date = datetime.datetime.fromtimestamp(new['datetime']).strftime('%Y-%m-%d')
          encode_news_df.loc[date, сoncurrency_ticker] += new['summary']

memory_usage = encode_news_df.memory_usage(deep=True)

# Total memory usage of the DataFrame
total_memory_usage = memory_usage.sum() / (1024 * 1024)  # Convert to MB
print("Memory usage of the DataFrame:", total_memory_usage, "MB")
print(encode_news_df.head())

# Получение эмбеддингов текста
with torch.no_grad():
    encode_news_df = encode_news_df.apply(lambda x: roberta(torch.tensor(tokenizer.encode(x, add_special_tokens=True))
                                          .long()
                                          .unsqueeze(0))
                                          .last_hidden_state.squeeze(0))
    encode_news_df = encode_news_df.applymap(lambda x: np.pad(x, (0, max_len - len(x)), 'constant', constant_values=0))

"""## Создание нейронной сети и пронозирование"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATv2Conv

class GAT(nn.Module):
    def __init__(self, num_features, num_classes, hidden_layer, num_heads):
        super(GAT, self).__init__()

        self.num_heads = num_heads
        # Слой графового внимания GAT
        self.conv1 = GATv2Conv(num_features, hidden_layer, heads=num_heads)
        # Полносвязный слой для классификации
        self.fc = nn.Linear(hidden_layer * num_heads, num_classes)

    def forward(self, x, edge_index, edge_attr):
        # Применение слоя графового внимания GAT
        x = self.conv1(x, edge_index, edge_attr=edge_attr)
        # Применение функции активации LeakyReLU
        x = F.leaky_relu(x)
        # Вычисление среднего значения по головам в слое графового внимания
        x = torch.mean(x.view(-1, self.num_heads, 32), dim=1)
        # Применение полносвязного слоя для классификации
        x = self.fc(x)
        # Применение функции активации softmax для получения вероятностей классов
        x = F.softmax(x, dim=1)

        return x